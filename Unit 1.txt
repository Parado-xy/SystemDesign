Three important aspects of Software:
- Reliability;
- Scalability;
- Maintainability;


- RELIABILITY;
 Everybody has an intuitive idea of what it means for something to be reliable or unre
liable. For software, typical expectations include:
 • The application performs the function that the user expected.
 • It can tolerate the user making mistakes or using the software in unexpected
 ways.
 • Its performance is good enough for the required use case, under the expected
 load and data volume.
 • The system prevents any unauthorized access and abuse.

If all those things together mean “working correctly,” then we can understand relia
bility as meaning, roughly, “continuing to work correctly, even when things go
 wrong.”

The things that can go wrong are called faults, and systems that anticipate faults and
 can cope with them are called fault-tolerant or resilient.

Note that a fault is not the same as a failure. A fault is usually defined as one com
ponent of the system deviating from its spec, whereas a failure is when the system as a
 whole stops providing the required service to the user. It is impossible to reduce the
 probability of a fault to zero; therefore it is usually best to design fault-tolerance
 mechanisms that prevent faults from causing failures.

- HARDWARE FAULTS;
 When we think of causes of system failure, hardware faults quickly come to mind.
 Hard disks crash, RAM becomes faulty, the power grid has a blackout, someone
 unplugs the wrong network cable. Anyone who has worked with large datacenters
 can tell you that these things happen all the time when you have a lot of machines.
 Hard disks are reported as having a mean time to failure (MTTF) of about 10 to 50
 years [5, 6]. Thus, on a storage cluster with 10,000 disks, we should expect on average
 one disk to die per day.

- SOFTWARE ERRORS;
 We usually think of hardware faults as being random and independent from each
 other: one machine’s disk failing does not imply that another machine’s disk is going
 to fail. There may be weak correlations (for example due to a common cause, such as
 the temperature in the server rack), but otherwise it is unlikely that a large number of
 hardware components will fail at the same time.
 Another class of fault is a systematic error within the system [8]. Such faults are
 harder to anticipate, and because they are correlated across nodes, they tend to cause
 many more system failures than uncorrelated hardware faults [5]. Examples include:
 • A software bug that causes every instance of an application server to crash when
 given a particular bad input. For example, consider the leap second on June 30,
 2012, that caused many applications to hang simultaneously due to a bug in the
 Linux kernel [9].
 • A runaway process that uses up some shared resource—CPU time, memory, disk
 space, or network bandwidth.
• A service that the system depends on that slows down, becomes unresponsive, or
 starts returning corrupted responses.
 • Cascading failures, where a small fault in one component triggers a fault in
 another component, which in turn triggers further faults [10].
 The bugs that cause these kinds of software faults often lie dormant for a long time
 until they are triggered by an unusual set of circumstances. In those circumstances, it
 is revealed that the software is making some kind of assumption about its environ
ment—and while that assumption is usually true, it eventually stops being true for
 some reason [11].
 There is no quick solution to the problem of systematic faults in software. Lots of
 small things can help: carefully thinking about assumptions and interactions in the
 system; thorough testing; process isolation; allowing processes to crash and restart;
 measuring, monitoring, and analyzing system behavior in production. If a system is
 expected to provide some guarantee (for example, in a message queue, that the num
ber of incoming messages equals the number of outgoing messages), it can constantly
 check itself while it is running and raise an alert if a discrepancy is found [12]

- HUMAN ERRORS;
Humans design and build software systems, and the operators who keep the systems
running are also human. Even when they have the best intentions, humans are
known to be unreliable. For example, one study of large internet services found that
configuration errors by operators were the leading cause of outages, whereas hard
ware faults (servers or network) played a role in only 10–25% of outages





- SCALABILITY;
Even if a system is working reliably today, that doesn’t mean it will necessarily work
reliably in the future. One common reason for degradation is increased load: perhaps
the system has grown from 10,000 concurrent users to 100,000 concurrent users, or
from 1 million to 10 million. Perhaps it is processing much larger volumes of data
than it did before.
Scalability is the term we use to describe a system’s ability to cope with increased
load

- DESCRIBING LOAD;
First, we need to succinctly describe the current load on the system; only then can we
discuss growth questions (what happens if our load doubles?). Load can be described
with a few numbers which we call load parameters. The best choice of parameters
depends on the architecture of your system: it may be requests per second to a web
server, the ratio of reads to writes in a database, the number of simultaneously active
users in a chat room, the hit rate on a cache, or something else. Perhaps the average
case is what matters for you, or perhaps your bottleneck is dominated by a small
number of extreme cases.


- DESCRIBING PERFORMANCE;
Once you have described the load on your system, you can investigate what happens
when the load increases. You can look at it in two ways:
 • When you increase a load parameter and keep the system resources (CPU, mem
ory, network bandwidth, etc.) unchanged, how is the performance of your system
 affected?
 • When you increase a load parameter, how much do you need to increase the
 resources if you want to keep performance unchanged?


- LATENCY AND RESPONSE TIME;
Latency and response time are often used synonymously, but they are not the same. The response time is what the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays. Latency is the duration that a request is waiting to be handled—during which it is latent, awaiting service 


It’s common to see the average response time of a service reported. (Strictly speaking, the term “average” doesn’t refer to any particular formula, but in practice it is usually understood as the arithmetic mean: given n values, add up all the values, and divideby n.) However, the mean is not a very good metric if you want to know your “typical” response time, because it doesn’t tell you how many users actually experienced that delay. Usually it is better to use percentiles. If you take your list of response times and sort it from fastest to slowest, then the median is the halfway point: for example, if your median response time is 200 ms, that means half your requests return in less than
 200 ms, and half your requests take longer than that. This makes the median a good metric if you want to know how long users typically have to wait: half of user requests are served in less than the median response time, and the other half take longer than the median. The median is also known as the 50th percentile, and sometimes abbreviated as p50. Note that the median refers to a single request; if the user makes several requests (over the course of a session, or because several resources are included in a single page), the probability that at least one of them is slower than the median is much greater than 50%. In order to figure out how bad your outliers are, you can look at higher percentiles: the 95th, 99th, and 99.9th percentiles are common (abbreviated p95, p99, and p999). They are the response time thresholds at which 95%, 99%, or 99.9% of requests are faster than that particular threshold. For example, if the 95th percentile response time is 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds, and 5 out of 100 requests take 1.5 seconds or more. 

High percentiles of response times, also known as tail latencies, are important because they directly affect users’ experience of the service. For example, Amazon describes response time requirements for internal services in terms of the 99.9th percentile, even though it only affects 1 in 1,000 requests. This is because the customers with the slowest requests are often those who have the most data on their accounts because they have made many purchases—that is, they’re the most valuable customers. It’s important to keep those customers happy by ensuring the website is fast for them: Amazon has also observed that a 100 ms increase in response time reduces sales by 1% [20], and others report that a 1-second slowdown reduces a customer satisfaction metric by 16%.
















