Vertical scaling vs horizontal scaling.

Vertical scaling, referred to as “scale up”, means the process of adding more power (CPU,
RAM, etc.) to your servers. Horizontal scaling, referred to as “scale-out”, allows you to scale
by adding more servers into your pool of resources.
When traffic is low, vertical scaling is a great option, and the simplicity of vertical scaling is
its main advantage. Unfortunately, it comes with serious limitations.
• Vertical scaling has a hard limit. It is impossible to add unlimited CPU and memory to a
single server.
• Vertical scaling does not have failover and redundancy. If one server goes down, the
website/app goes down with it completely.
Horizontal scaling is more desirable for large scale applications due to the limitations of
vertical scaling.
In the previous design, users are connected to the web server directly. Users will be unable to
access the website if the web server is offline. In another scenario, if many users access the
web server simultaneously and it reaches the web server’s load limit, users generally
experience slower response or fail to connect to the server. A load balancer is the best
technique to address these problems.


Load balancer

A load balancer evenly distributes incoming traffic among web servers that are defined in a
load-balanced set. Figure 1-4 shows how a load balancer works.
As shown in Figure 1-4, users connect to the public IP of the load balancer directly. With this
setup, web servers are unreachable directly by clients anymore. For better security, private
IPs are used for communication between servers. A private IP is an IP address reachable only
between servers in the same network; however, it is unreachable over the internet. The load
balancer communicates with web servers through private IPs.
In Figure 1-4, after a load balancer and a second web server are added, we successfully
solved no failover issue and improved the availability of the web tier. Details are explained
below:
• If server 1 goes offline, all the traffic will be routed to server 2. This prevents the website
from going offline. We will also add a new healthy web server to the server pool to
balance the load.
• If the website traffic grows rapidly, and two servers are not enough to handle the traffic,
the load balancer can handle this problem gracefully. You only need to add more servers
to the web server pool, and the load balancer automatically starts to send requests to them.

Database replication

Quoted from Wikipedia: “Database replication can be used in many database management
systems, usually with a master/slave relationship between the original (master) and the copies
(slaves)” [3].
A master database generally only supports write operations. A slave database gets copies of
the data from the master database and only supports read operations. All the data-modifying
commands like insert, delete, or update must be sent to the master database. Most
applications require a much higher ratio of reads to writes; thus, the number of slave
databases in a system is usually larger than the number of master databases. Figure 1-5 shows
a master database with multiple slave databases.
Advantages of database replication:
• Better performance: In the master-slave model, all writes and updates happen in master
nodes; whereas, read operations are distributed across slave nodes. This model improves
performance because it allows more queries to be processed in parallel.
• Reliability: If one of your database servers is destroyed by a natural disaster, such as a
typhoon or an earthquake, data is still preserved. You do not need to worry about data loss
because data is replicated across multiple locations.
• High availability: By replicating data across different locations, your website remains in
operation even if a database is offline as you can access data stored in another database
server.
In the previous section, we discussed how a load balancer helped to improve system
availability. We ask the same question here: what if one of the databases goes offline? The
architectural design discussed in Figure 1-5 can handle this case:
• If only one slave database is available and it goes offline, read operations will be directed
to the master database temporarily. As soon as the issue is found, a new slave database
will replace the old one. In case multiple slave databases are available, read operations are
redirected to other healthy slave databases. A new database server will replace the old one.
• If the master database goes offline, a slave database will be promoted to be the new
master. All the database operations will be temporarily executed on the new master
database. A new slave database will replace the old one for data replication immediately.
In production systems, promoting a new master is more complicated as the data in a slave
database might not be up to date. The missing data needs to be updated by running data
recovery scripts. Although some other replication methods like multi-masters and circular
replication could help, those setups are more complicated; and their discussions are
beyond the scope of this book. Interested readers should refer to the listed reference
materials.

Now, you have a solid understanding of the web and data tiers, it is time to improve the
load/response time. This can be done by adding a cache layer and shifting static content
(JavaScript/CSS/image/video files) to the content delivery network (CDN).

Cache
A cache is a temporary storage area that stores the result of expensive responses or frequently
accessed data in memory so that subsequent requests are served more quickly. Every time a new web page loads, one or more database calls are executed to
fetch data. The application performance is greatly affected by calling the database repeatedly.
The cache can mitigate this problem.

Cache tier
The cache tier is a temporary data store layer, much faster than the database. The benefits of
having a separate cache tier include better system performance, ability to reduce database
workloads, and the ability to scale the cache tier independently. Figure 1-7 shows a possible
setup of a cache server:

After receiving a request, a web server first checks if the cache has the available response. If
it has, it sends data back to the client. If not, it queries the database, stores the response in
cache, and sends it back to the client. This caching strategy is called a read-through cache.
Other caching strategies are available depending on the data type, size, and access patterns. A
previous study explains how different caching strategies work [6].
Interacting with cache servers is simple because most cache servers provide APIs for
common programming languages. The following code snippet shows typical Memcached

SECONDS = 1
cache.set('myKey', 'hi there', 3600 * SECONDS)
cache.get('myKey')

Considerations for using cache
Here are a few considerations for using a cache system:
• Decide when to use cache. Consider using cache when data is read frequently but
modified infrequently. Since cached data is stored in volatile memory, a cache server is
not ideal for persisting data. For instance, if a cache server restarts, all the data in memory
is lost. Thus, important data should be saved in persistent data stores.
• Expiration policy. It is a good practice to implement an expiration policy. Once cached
data is expired, it is removed from the cache. When there is no expiration policy, cached
data will be stored in the memory permanently. It is advisable not to make the expiration
date too short as this will cause the system to reload data from the database too frequently.
Meanwhile, it is advisable not to make the expiration date too long as the data can become
stale.
• Consistency: This involves keeping the data store and the cache in sync. Inconsistency
can happen because data-modifying operations on the data store and cache are not in a
single transaction. When scaling across multiple regions, maintaining consistency between
the data store and cache is challenging. For further details, refer to the paper titled
“Scaling Memcache at Facebook” published by Facebook [7].
• Mitigating failures: A single cache server represents a potential single point of failure
(SPOF), defined in Wikipedia as follows: “A single point of failure (SPOF) is a part of a
system that, if it fails, will stop the entire system from working” [8]. As a result, multiple
cache servers across different data centers are recommended to avoid SPOF. Another
recommended approach is to overprovision the required memory by certain percentages.
This provides a buffer as the memory usage increases.
• Eviction Policy: Once the cache is full, any requests to add items to the cache might
cause existing items to be removed. This is called cache eviction. Least-recently-used
(LRU) is the most popular cache eviction policy. Other eviction policies, such as the Least
Frequently Used (LFU) or First in First Out (FIFO), can be adopted to satisfy different use
cases.

Content delivery network (CDN) 
A CDN is a network of geographically dispersed servers used to deliver static content. CDN
servers cache static content like images, videos, CSS, JavaScript files, etc.
Dynamic content caching is a relatively new concept and beyond the scope of this book. It
enables the caching of HTML pages that are based on request path, query strings, cookies,
and request headers. Refer to the article mentioned in reference material [9] for more about
this. 
Here is how CDN works at the high-level: when a user visits a website, a CDN server closest
to the user will deliver static content. Intuitively, the further users are from CDN servers, the
slower the website loads. For example, if CDN servers are in San Francisco, users in Los
Angeles will get content faster than users in Europe. Figure 1-9 is a great example that shows
how CDN improves load time.

While CDNs and caching are both used to improve website performance, they have distinct differences:

**CDN (Content Delivery Network):**

* **Geographically distributed network:** A CDN is a network of servers located in various data centers around the world.
* **Content distribution:** It stores copies of your website's static content (images, CSS, JavaScript) on these servers.
* **Faster delivery:** When a user accesses your website, the CDN serves the content from the nearest server, reducing latency and improving load times.
* **Additional features:** CDNs often offer additional features like load balancing, security, and DDoS protection.

**Caching:**

* **Local storage:** Caching involves storing a copy of frequently accessed content on a user's device or server.
* **Reduced server load:** This reduces the need to fetch the content from the origin server each time, improving performance and reducing server load.
* **Types of caching:** Caching can occur at various levels, including browser caching, server-side caching, and CDN caching.

**Relationship between CDN and Caching:**

* CDNs utilize caching to store content on their edge servers, which are geographically distributed.
* Caching is a key component of how CDNs achieve faster delivery and reduced server load.

**In summary:**

* **CDNs** provide a global network for distributing content.
* **Caching** is a technique for storing content locally to improve performance.

Both CDNs and caching work together to optimize website performance and user experience.


AN INTERACTION BETWEEN GEMINI AND MYSELF:

PROMPT: So if i'm getting this right, if we had a system, if users made requests for static files, those requests would be served from a CDN,
but when they make API requests, the server responds..? 
The server can choose to cache certain values based on the frequency of the request for that value so it doesn't have to keep quering  the database..?

ANSWER: You've got it exactly right! Here's a breakdown of how CDNs and caching work together in this scenario:

**Static File Requests:**

1. **User Request:** A user requests a static file like an image, CSS file, or JavaScript file.
2. **CDN Intercept:** The CDN intercepts the request.
3. **Cached or Origin Server:** If the file is cached on a nearby CDN server, it's served directly from there, significantly reducing load times. 
If not, the CDN fetches the file from the origin server and caches it for future requests.

**API Requests:**

1. **User Request:** A user makes an API request to the server.
2. **Server Processing:** The server processes the request, which may involve querying a database or performing other calculations.
3. **Response Generation:** The server generates the response and sends it back to the user.
4. **Caching (Optional):** If the response is frequently requested and doesn't change often, the server can cache it. This means that subsequent requests for the same data can be served directly from the cache, bypassing the database query and speeding up response times.

**Key Points:**

* **CDNs excel at serving static content** like images, CSS, and JavaScript files.
* **Servers handle dynamic content** generated by API requests.
* **Caching** can be implemented at both the CDN and server levels to improve performance.

By combining CDNs and caching, you can significantly enhance the performance and scalability of your web applications.

